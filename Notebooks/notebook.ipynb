{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = ['*'] + sorted(list(set(''.join(words))))\n",
    "stoi = {ch: ix for ix, ch in enumerate(abc)}\n",
    "itos = {ix: ch for ch, ix in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "*** e\n",
      "[[0, 0, 0]] [5]\n",
      "**e m\n",
      "[[0, 0, 0], [0, 0, 5]] [5, 13]\n",
      "*em m\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13]] [5, 13, 13]\n",
      "emm a\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13]] [5, 13, 13, 1]\n",
      "mma *\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1]] [5, 13, 13, 1, 0]\n",
      "olivia\n",
      "*** o\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0]] [5, 13, 13, 1, 0, 15]\n",
      "**o l\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15]] [5, 13, 13, 1, 0, 15, 12]\n",
      "*ol i\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12]] [5, 13, 13, 1, 0, 15, 12, 9]\n",
      "oli v\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9]] [5, 13, 13, 1, 0, 15, 12, 9, 22]\n",
      "liv i\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22]] [5, 13, 13, 1, 0, 15, 12, 9, 22, 9]\n",
      "ivi a\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9]] [5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1]\n",
      "via *\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1]] [5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0]\n",
      "ava\n",
      "*** a\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0]] [5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1]\n",
      "**a v\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1]] [5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22]\n",
      "*av a\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22]] [5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1]\n",
      "ava *\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1]] [5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0]\n",
      "isabella\n",
      "*** i\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0]] [5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9]\n",
      "**i s\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9]] [5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19]\n",
      "*is a\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19]] [5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1]\n",
      "isa b\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1]] [5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2]\n",
      "sab e\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1], [19, 1, 2]] [5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5]\n",
      "abe l\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1], [19, 1, 2], [1, 2, 5]] [5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5, 12]\n",
      "bel l\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1], [19, 1, 2], [1, 2, 5], [2, 5, 12]] [5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5, 12, 12]\n",
      "ell a\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1], [19, 1, 2], [1, 2, 5], [2, 5, 12], [5, 12, 12]] [5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5, 12, 12, 1]\n",
      "lla *\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1], [19, 1, 2], [1, 2, 5], [2, 5, 12], [5, 12, 12], [12, 12, 1]] [5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5, 12, 12, 1, 0]\n",
      "sophia\n",
      "*** s\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1], [19, 1, 2], [1, 2, 5], [2, 5, 12], [5, 12, 12], [12, 12, 1], [0, 0, 0]] [5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5, 12, 12, 1, 0, 19]\n",
      "**s o\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1], [19, 1, 2], [1, 2, 5], [2, 5, 12], [5, 12, 12], [12, 12, 1], [0, 0, 0], [0, 0, 19]] [5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5, 12, 12, 1, 0, 19, 15]\n",
      "*so p\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1], [19, 1, 2], [1, 2, 5], [2, 5, 12], [5, 12, 12], [12, 12, 1], [0, 0, 0], [0, 0, 19], [0, 19, 15]] [5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5, 12, 12, 1, 0, 19, 15, 16]\n",
      "sop h\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1], [19, 1, 2], [1, 2, 5], [2, 5, 12], [5, 12, 12], [12, 12, 1], [0, 0, 0], [0, 0, 19], [0, 19, 15], [19, 15, 16]] [5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5, 12, 12, 1, 0, 19, 15, 16, 8]\n",
      "oph i\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1], [19, 1, 2], [1, 2, 5], [2, 5, 12], [5, 12, 12], [12, 12, 1], [0, 0, 0], [0, 0, 19], [0, 19, 15], [19, 15, 16], [15, 16, 8]] [5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5, 12, 12, 1, 0, 19, 15, 16, 8, 9]\n",
      "phi a\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1], [19, 1, 2], [1, 2, 5], [2, 5, 12], [5, 12, 12], [12, 12, 1], [0, 0, 0], [0, 0, 19], [0, 19, 15], [19, 15, 16], [15, 16, 8], [16, 8, 9]] [5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5, 12, 12, 1, 0, 19, 15, 16, 8, 9, 1]\n",
      "hia *\n",
      "[[0, 0, 0], [0, 0, 5], [0, 5, 13], [5, 13, 13], [13, 13, 1], [0, 0, 0], [0, 0, 15], [0, 15, 12], [15, 12, 9], [12, 9, 22], [9, 22, 9], [22, 9, 1], [0, 0, 0], [0, 0, 1], [0, 1, 22], [1, 22, 1], [0, 0, 0], [0, 0, 9], [0, 9, 19], [9, 19, 1], [19, 1, 2], [1, 2, 5], [2, 5, 12], [5, 12, 12], [12, 12, 1], [0, 0, 0], [0, 0, 19], [0, 19, 15], [19, 15, 16], [15, 16, 8], [16, 8, 9], [8, 9, 1]] [5, 13, 13, 1, 0, 15, 12, 9, 22, 9, 1, 0, 1, 22, 1, 0, 9, 19, 1, 2, 5, 12, 12, 1, 0, 19, 15, 16, 8, 9, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "X, Y = [], []\n",
    "for w in words[:5]:\n",
    "    print(w)\n",
    "    context = [0] * 3 \n",
    "    for ch in w + '*': \n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(itos[i] for i in context), ch)\n",
    "        print(X, Y)\n",
    "        context = context[1:] + [ix]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('n', '.'), 6763),\n",
       " (('a', '.'), 6640),\n",
       " (('a', 'n'), 5438),\n",
       " (('.', 'a'), 4410),\n",
       " (('e', '.'), 3983),\n",
       " (('a', 'r'), 3264),\n",
       " (('e', 'l'), 3248),\n",
       " (('r', 'i'), 3033),\n",
       " (('n', 'a'), 2977),\n",
       " (('.', 'k'), 2963),\n",
       " (('l', 'e'), 2921),\n",
       " (('e', 'n'), 2675),\n",
       " (('l', 'a'), 2623),\n",
       " (('m', 'a'), 2590),\n",
       " (('.', 'm'), 2538),\n",
       " (('a', 'l'), 2528),\n",
       " (('i', '.'), 2489),\n",
       " (('l', 'i'), 2480),\n",
       " (('i', 'a'), 2445),\n",
       " (('.', 'j'), 2422),\n",
       " (('o', 'n'), 2411),\n",
       " (('h', '.'), 2409),\n",
       " (('r', 'a'), 2356),\n",
       " (('a', 'h'), 2332),\n",
       " (('h', 'a'), 2244),\n",
       " (('y', 'a'), 2143),\n",
       " (('i', 'n'), 2126),\n",
       " (('.', 's'), 2055),\n",
       " (('a', 'y'), 2050),\n",
       " (('y', '.'), 2007),\n",
       " (('e', 'r'), 1958),\n",
       " (('n', 'n'), 1906),\n",
       " (('y', 'n'), 1826),\n",
       " (('k', 'a'), 1731),\n",
       " (('n', 'i'), 1725),\n",
       " (('r', 'e'), 1697),\n",
       " (('.', 'd'), 1690),\n",
       " (('i', 'e'), 1653),\n",
       " (('a', 'i'), 1650),\n",
       " (('.', 'r'), 1639),\n",
       " (('a', 'm'), 1634),\n",
       " (('l', 'y'), 1588),\n",
       " (('.', 'l'), 1572),\n",
       " (('.', 'c'), 1542),\n",
       " (('.', 'e'), 1531),\n",
       " (('j', 'a'), 1473),\n",
       " (('r', '.'), 1377),\n",
       " (('n', 'e'), 1359),\n",
       " (('l', 'l'), 1345),\n",
       " (('i', 'l'), 1345),\n",
       " (('i', 's'), 1316),\n",
       " (('l', '.'), 1314),\n",
       " (('.', 't'), 1308),\n",
       " (('.', 'b'), 1306),\n",
       " (('d', 'a'), 1303),\n",
       " (('s', 'h'), 1285),\n",
       " (('d', 'e'), 1283),\n",
       " (('e', 'e'), 1271),\n",
       " (('m', 'i'), 1256),\n",
       " (('s', 'a'), 1201),\n",
       " (('s', '.'), 1169),\n",
       " (('.', 'n'), 1146),\n",
       " (('a', 's'), 1118),\n",
       " (('y', 'l'), 1104),\n",
       " (('e', 'y'), 1070),\n",
       " (('o', 'r'), 1059),\n",
       " (('a', 'd'), 1042),\n",
       " (('t', 'a'), 1027),\n",
       " (('.', 'z'), 929),\n",
       " (('v', 'i'), 911),\n",
       " (('k', 'e'), 895),\n",
       " (('s', 'e'), 884),\n",
       " (('.', 'h'), 874),\n",
       " (('r', 'o'), 869),\n",
       " (('e', 's'), 861),\n",
       " (('z', 'a'), 860),\n",
       " (('o', '.'), 855),\n",
       " (('i', 'r'), 849),\n",
       " (('b', 'r'), 842),\n",
       " (('a', 'v'), 834),\n",
       " (('m', 'e'), 818),\n",
       " (('e', 'i'), 818),\n",
       " (('c', 'a'), 815),\n",
       " (('i', 'y'), 779),\n",
       " (('r', 'y'), 773),\n",
       " (('e', 'm'), 769),\n",
       " (('s', 't'), 765),\n",
       " (('h', 'i'), 729),\n",
       " (('t', 'e'), 716),\n",
       " (('n', 'd'), 704),\n",
       " (('l', 'o'), 692),\n",
       " (('a', 'e'), 692),\n",
       " (('a', 't'), 687),\n",
       " (('s', 'i'), 684),\n",
       " (('e', 'a'), 679),\n",
       " (('d', 'i'), 674),\n",
       " (('h', 'e'), 674),\n",
       " (('.', 'g'), 669),\n",
       " (('t', 'o'), 667),\n",
       " (('c', 'h'), 664),\n",
       " (('b', 'e'), 655),\n",
       " (('t', 'h'), 647),\n",
       " (('v', 'a'), 642),\n",
       " (('o', 'l'), 619),\n",
       " (('.', 'i'), 591),\n",
       " (('i', 'o'), 588),\n",
       " (('e', 't'), 580),\n",
       " (('v', 'e'), 568),\n",
       " (('a', 'k'), 568),\n",
       " (('a', 'a'), 556),\n",
       " (('c', 'e'), 551),\n",
       " (('a', 'b'), 541),\n",
       " (('i', 't'), 541),\n",
       " (('.', 'y'), 535),\n",
       " (('t', 'i'), 532),\n",
       " (('s', 'o'), 531),\n",
       " (('m', '.'), 516),\n",
       " (('d', '.'), 516),\n",
       " (('.', 'p'), 515),\n",
       " (('i', 'c'), 509),\n",
       " (('k', 'i'), 509),\n",
       " (('o', 's'), 504),\n",
       " (('n', 'o'), 496),\n",
       " (('t', '.'), 483),\n",
       " (('j', 'o'), 479),\n",
       " (('u', 's'), 474),\n",
       " (('a', 'c'), 470),\n",
       " (('n', 'y'), 465),\n",
       " (('e', 'v'), 463),\n",
       " (('s', 's'), 461),\n",
       " (('m', 'o'), 452),\n",
       " (('i', 'k'), 445),\n",
       " (('n', 't'), 443),\n",
       " (('i', 'd'), 440),\n",
       " (('j', 'e'), 440),\n",
       " (('a', 'z'), 435),\n",
       " (('i', 'g'), 428),\n",
       " (('i', 'm'), 427),\n",
       " (('r', 'r'), 425),\n",
       " (('d', 'r'), 424),\n",
       " (('.', 'f'), 417),\n",
       " (('u', 'r'), 414),\n",
       " (('r', 'l'), 413),\n",
       " (('y', 's'), 401),\n",
       " (('.', 'o'), 394),\n",
       " (('e', 'd'), 384),\n",
       " (('a', 'u'), 381),\n",
       " (('c', 'o'), 380),\n",
       " (('k', 'y'), 379),\n",
       " (('d', 'o'), 378),\n",
       " (('.', 'v'), 376),\n",
       " (('t', 't'), 374),\n",
       " (('z', 'e'), 373),\n",
       " (('z', 'i'), 364),\n",
       " (('k', '.'), 363),\n",
       " (('g', 'h'), 360),\n",
       " (('t', 'r'), 352),\n",
       " (('k', 'o'), 344),\n",
       " (('t', 'y'), 341),\n",
       " (('g', 'e'), 334),\n",
       " (('g', 'a'), 330),\n",
       " (('l', 'u'), 324),\n",
       " (('b', 'a'), 321),\n",
       " (('d', 'y'), 317),\n",
       " (('c', 'k'), 316),\n",
       " (('.', 'w'), 307),\n",
       " (('k', 'h'), 307),\n",
       " (('u', 'l'), 301),\n",
       " (('y', 'e'), 301),\n",
       " (('y', 'r'), 291),\n",
       " (('m', 'y'), 287),\n",
       " (('h', 'o'), 287),\n",
       " (('w', 'a'), 280),\n",
       " (('s', 'l'), 279),\n",
       " (('n', 's'), 278),\n",
       " (('i', 'z'), 277),\n",
       " (('u', 'n'), 275),\n",
       " (('o', 'u'), 275),\n",
       " (('n', 'g'), 273),\n",
       " (('y', 'd'), 272),\n",
       " (('c', 'i'), 271),\n",
       " (('y', 'o'), 271),\n",
       " (('i', 'v'), 269),\n",
       " (('e', 'o'), 269),\n",
       " (('o', 'm'), 261),\n",
       " (('r', 'u'), 252),\n",
       " (('f', 'a'), 242),\n",
       " (('b', 'i'), 217),\n",
       " (('s', 'y'), 215),\n",
       " (('n', 'c'), 213),\n",
       " (('h', 'y'), 213),\n",
       " (('p', 'a'), 209),\n",
       " (('r', 't'), 208),\n",
       " (('q', 'u'), 206),\n",
       " (('p', 'h'), 204),\n",
       " (('h', 'r'), 204),\n",
       " (('j', 'u'), 202),\n",
       " (('g', 'r'), 201),\n",
       " (('p', 'e'), 197),\n",
       " (('n', 'l'), 195),\n",
       " (('y', 'i'), 192),\n",
       " (('g', 'i'), 190),\n",
       " (('o', 'd'), 190),\n",
       " (('r', 's'), 190),\n",
       " (('r', 'd'), 187),\n",
       " (('h', 'l'), 185),\n",
       " (('s', 'u'), 185),\n",
       " (('a', 'x'), 182),\n",
       " (('e', 'z'), 181),\n",
       " (('e', 'k'), 178),\n",
       " (('o', 'v'), 176),\n",
       " (('a', 'j'), 175),\n",
       " (('o', 'h'), 171),\n",
       " (('u', 'e'), 169),\n",
       " (('m', 'm'), 168),\n",
       " (('a', 'g'), 168),\n",
       " (('h', 'u'), 166),\n",
       " (('x', '.'), 164),\n",
       " (('u', 'a'), 163),\n",
       " (('r', 'm'), 162),\n",
       " (('a', 'w'), 161),\n",
       " (('f', 'i'), 160),\n",
       " (('z', '.'), 160),\n",
       " (('u', '.'), 155),\n",
       " (('u', 'm'), 154),\n",
       " (('e', 'c'), 153),\n",
       " (('v', 'o'), 153),\n",
       " (('e', 'h'), 152),\n",
       " (('p', 'r'), 151),\n",
       " (('d', 'd'), 149),\n",
       " (('o', 'a'), 149),\n",
       " (('w', 'e'), 149),\n",
       " (('w', 'i'), 148),\n",
       " (('y', 'm'), 148),\n",
       " (('z', 'y'), 147),\n",
       " (('n', 'z'), 145),\n",
       " (('y', 'u'), 141),\n",
       " (('r', 'n'), 140),\n",
       " (('o', 'b'), 140),\n",
       " (('k', 'l'), 139),\n",
       " (('m', 'u'), 139),\n",
       " (('l', 'd'), 138),\n",
       " (('h', 'n'), 138),\n",
       " (('u', 'd'), 136),\n",
       " (('.', 'x'), 134),\n",
       " (('t', 'l'), 134),\n",
       " (('a', 'f'), 134),\n",
       " (('o', 'e'), 132),\n",
       " (('e', 'x'), 132),\n",
       " (('e', 'g'), 125),\n",
       " (('f', 'e'), 123),\n",
       " (('z', 'l'), 123),\n",
       " (('u', 'i'), 121),\n",
       " (('v', 'y'), 121),\n",
       " (('e', 'b'), 121),\n",
       " (('r', 'h'), 121),\n",
       " (('j', 'i'), 119),\n",
       " (('o', 't'), 118),\n",
       " (('d', 'h'), 118),\n",
       " (('h', 'm'), 117),\n",
       " (('c', 'l'), 116),\n",
       " (('o', 'o'), 115),\n",
       " (('y', 'c'), 115),\n",
       " (('o', 'w'), 114),\n",
       " (('o', 'c'), 114),\n",
       " (('f', 'r'), 114),\n",
       " (('b', '.'), 114),\n",
       " (('m', 'b'), 112),\n",
       " (('z', 'o'), 110),\n",
       " (('i', 'b'), 110),\n",
       " (('i', 'u'), 109),\n",
       " (('k', 'r'), 109),\n",
       " (('g', '.'), 108),\n",
       " (('y', 'v'), 106),\n",
       " (('t', 'z'), 105),\n",
       " (('b', 'o'), 105),\n",
       " (('c', 'y'), 104),\n",
       " (('y', 't'), 104),\n",
       " (('u', 'b'), 103),\n",
       " (('u', 'c'), 103),\n",
       " (('x', 'a'), 103),\n",
       " (('b', 'l'), 103),\n",
       " (('o', 'y'), 103),\n",
       " (('x', 'i'), 102),\n",
       " (('i', 'f'), 101),\n",
       " (('r', 'c'), 99),\n",
       " (('c', '.'), 97),\n",
       " (('m', 'r'), 97),\n",
       " (('n', 'u'), 96),\n",
       " (('o', 'p'), 95),\n",
       " (('i', 'h'), 95),\n",
       " (('k', 's'), 95),\n",
       " (('l', 's'), 94),\n",
       " (('u', 'k'), 93),\n",
       " (('.', 'q'), 92),\n",
       " (('d', 'u'), 92),\n",
       " (('s', 'm'), 90),\n",
       " (('r', 'k'), 90),\n",
       " (('i', 'x'), 89),\n",
       " (('v', '.'), 88),\n",
       " (('y', 'k'), 86),\n",
       " (('u', 'w'), 86),\n",
       " (('g', 'u'), 85),\n",
       " (('b', 'y'), 83),\n",
       " (('e', 'p'), 83),\n",
       " (('g', 'o'), 83),\n",
       " (('s', 'k'), 82),\n",
       " (('u', 't'), 82),\n",
       " (('a', 'p'), 82),\n",
       " (('e', 'f'), 82),\n",
       " (('i', 'i'), 82),\n",
       " (('r', 'v'), 80),\n",
       " (('f', '.'), 80),\n",
       " (('t', 'u'), 78),\n",
       " (('y', 'z'), 78),\n",
       " (('.', 'u'), 78),\n",
       " (('l', 't'), 77),\n",
       " (('r', 'g'), 76),\n",
       " (('c', 'r'), 76),\n",
       " (('i', 'j'), 76),\n",
       " (('w', 'y'), 73),\n",
       " (('z', 'u'), 73),\n",
       " (('l', 'v'), 72),\n",
       " (('h', 't'), 71),\n",
       " (('j', '.'), 71),\n",
       " (('x', 't'), 70),\n",
       " (('o', 'i'), 69),\n",
       " (('e', 'u'), 69),\n",
       " (('o', 'k'), 68),\n",
       " (('b', 'd'), 65),\n",
       " (('a', 'o'), 63),\n",
       " (('p', 'i'), 61),\n",
       " (('s', 'c'), 60),\n",
       " (('d', 'l'), 60),\n",
       " (('l', 'm'), 60),\n",
       " (('a', 'q'), 60),\n",
       " (('f', 'o'), 60),\n",
       " (('p', 'o'), 59),\n",
       " (('n', 'k'), 58),\n",
       " (('w', 'n'), 58),\n",
       " (('u', 'h'), 58),\n",
       " (('e', 'j'), 55),\n",
       " (('n', 'v'), 55),\n",
       " (('s', 'r'), 55),\n",
       " (('o', 'z'), 54),\n",
       " (('i', 'p'), 53),\n",
       " (('l', 'b'), 52),\n",
       " (('i', 'q'), 52),\n",
       " (('w', '.'), 51),\n",
       " (('m', 'c'), 51),\n",
       " (('s', 'p'), 51),\n",
       " (('e', 'w'), 50),\n",
       " (('k', 'u'), 50),\n",
       " (('v', 'r'), 48),\n",
       " (('u', 'g'), 47),\n",
       " (('o', 'x'), 45),\n",
       " (('u', 'z'), 45),\n",
       " (('z', 'z'), 45),\n",
       " (('j', 'h'), 45),\n",
       " (('b', 'u'), 45),\n",
       " (('o', 'g'), 44),\n",
       " (('n', 'r'), 44),\n",
       " (('f', 'f'), 44),\n",
       " (('n', 'j'), 44),\n",
       " (('z', 'h'), 43),\n",
       " (('c', 'c'), 42),\n",
       " (('r', 'b'), 41),\n",
       " (('x', 'o'), 41),\n",
       " (('b', 'h'), 41),\n",
       " (('p', 'p'), 39),\n",
       " (('x', 'l'), 39),\n",
       " (('h', 'v'), 39),\n",
       " (('b', 'b'), 38),\n",
       " (('m', 'p'), 38),\n",
       " (('x', 'x'), 38),\n",
       " (('u', 'v'), 37),\n",
       " (('x', 'e'), 36),\n",
       " (('w', 'o'), 36),\n",
       " (('c', 't'), 35),\n",
       " (('z', 'm'), 35),\n",
       " (('t', 's'), 35),\n",
       " (('m', 's'), 35),\n",
       " (('c', 'u'), 35),\n",
       " (('o', 'f'), 34),\n",
       " (('u', 'x'), 34),\n",
       " (('k', 'w'), 34),\n",
       " (('p', '.'), 33),\n",
       " (('g', 'l'), 32),\n",
       " (('z', 'r'), 32),\n",
       " (('d', 'n'), 31),\n",
       " (('g', 't'), 31),\n",
       " (('g', 'y'), 31),\n",
       " (('h', 's'), 31),\n",
       " (('x', 's'), 31),\n",
       " (('g', 's'), 30),\n",
       " (('x', 'y'), 30),\n",
       " (('y', 'g'), 30),\n",
       " (('d', 'm'), 30),\n",
       " (('d', 's'), 29),\n",
       " (('h', 'k'), 29),\n",
       " (('y', 'x'), 28),\n",
       " (('q', '.'), 28),\n",
       " (('g', 'n'), 27),\n",
       " (('y', 'b'), 27),\n",
       " (('g', 'w'), 26),\n",
       " (('n', 'h'), 26),\n",
       " (('k', 'n'), 26),\n",
       " (('g', 'g'), 25),\n",
       " (('d', 'g'), 25),\n",
       " (('l', 'c'), 25),\n",
       " (('r', 'j'), 25),\n",
       " (('w', 'u'), 25),\n",
       " (('l', 'k'), 24),\n",
       " (('m', 'd'), 24),\n",
       " (('s', 'w'), 24),\n",
       " (('s', 'n'), 24),\n",
       " (('h', 'd'), 24),\n",
       " (('w', 'h'), 23),\n",
       " (('y', 'j'), 23),\n",
       " (('y', 'y'), 23),\n",
       " (('r', 'z'), 23),\n",
       " (('d', 'w'), 23),\n",
       " (('w', 'r'), 22),\n",
       " (('t', 'n'), 22),\n",
       " (('l', 'f'), 22),\n",
       " (('y', 'h'), 22),\n",
       " (('r', 'w'), 21),\n",
       " (('s', 'b'), 21),\n",
       " (('m', 'n'), 20),\n",
       " (('f', 'l'), 20),\n",
       " (('w', 's'), 20),\n",
       " (('k', 'k'), 20),\n",
       " (('h', 'z'), 20),\n",
       " (('g', 'd'), 19),\n",
       " (('l', 'h'), 19),\n",
       " (('n', 'm'), 19),\n",
       " (('x', 'z'), 19),\n",
       " (('u', 'f'), 19),\n",
       " (('f', 't'), 18),\n",
       " (('l', 'r'), 18),\n",
       " (('p', 't'), 17),\n",
       " (('t', 'c'), 17),\n",
       " (('k', 't'), 17),\n",
       " (('d', 'v'), 17),\n",
       " (('u', 'p'), 16),\n",
       " (('p', 'l'), 16),\n",
       " (('l', 'w'), 16),\n",
       " (('p', 's'), 16),\n",
       " (('o', 'j'), 16),\n",
       " (('r', 'q'), 16),\n",
       " (('y', 'p'), 15),\n",
       " (('l', 'p'), 15),\n",
       " (('t', 'v'), 15),\n",
       " (('r', 'p'), 14),\n",
       " (('l', 'n'), 14),\n",
       " (('e', 'q'), 14),\n",
       " (('f', 'y'), 14),\n",
       " (('s', 'v'), 14),\n",
       " (('u', 'j'), 14),\n",
       " (('v', 'l'), 14),\n",
       " (('q', 'a'), 13),\n",
       " (('u', 'y'), 13),\n",
       " (('q', 'i'), 13),\n",
       " (('w', 'l'), 13),\n",
       " (('p', 'y'), 12),\n",
       " (('y', 'f'), 12),\n",
       " (('c', 'q'), 11),\n",
       " (('j', 'r'), 11),\n",
       " (('n', 'w'), 11),\n",
       " (('n', 'f'), 11),\n",
       " (('t', 'w'), 11),\n",
       " (('m', 'z'), 11),\n",
       " (('u', 'o'), 10),\n",
       " (('f', 'u'), 10),\n",
       " (('l', 'z'), 10),\n",
       " (('h', 'w'), 10),\n",
       " (('u', 'q'), 10),\n",
       " (('j', 'y'), 10),\n",
       " (('s', 'z'), 10),\n",
       " (('s', 'd'), 9),\n",
       " (('j', 'l'), 9),\n",
       " (('d', 'j'), 9),\n",
       " (('k', 'm'), 9),\n",
       " (('r', 'f'), 9),\n",
       " (('h', 'j'), 9),\n",
       " (('v', 'n'), 8),\n",
       " (('n', 'b'), 8),\n",
       " (('i', 'w'), 8),\n",
       " (('h', 'b'), 8),\n",
       " (('b', 's'), 8),\n",
       " (('w', 't'), 8),\n",
       " (('w', 'd'), 8),\n",
       " (('v', 'v'), 7),\n",
       " (('v', 'u'), 7),\n",
       " (('j', 's'), 7),\n",
       " (('m', 'j'), 7),\n",
       " (('f', 's'), 6),\n",
       " (('l', 'g'), 6),\n",
       " (('l', 'j'), 6),\n",
       " (('j', 'w'), 6),\n",
       " (('n', 'x'), 6),\n",
       " (('y', 'q'), 6),\n",
       " (('w', 'k'), 6),\n",
       " (('g', 'm'), 6),\n",
       " (('x', 'u'), 5),\n",
       " (('m', 'h'), 5),\n",
       " (('m', 'l'), 5),\n",
       " (('j', 'm'), 5),\n",
       " (('c', 's'), 5),\n",
       " (('j', 'v'), 5),\n",
       " (('n', 'p'), 5),\n",
       " (('d', 'f'), 5),\n",
       " (('x', 'd'), 5),\n",
       " (('z', 'b'), 4),\n",
       " (('f', 'n'), 4),\n",
       " (('x', 'c'), 4),\n",
       " (('m', 't'), 4),\n",
       " (('t', 'm'), 4),\n",
       " (('z', 'n'), 4),\n",
       " (('z', 't'), 4),\n",
       " (('p', 'u'), 4),\n",
       " (('c', 'z'), 4),\n",
       " (('b', 'n'), 4),\n",
       " (('z', 's'), 4),\n",
       " (('f', 'w'), 4),\n",
       " (('d', 't'), 4),\n",
       " (('j', 'd'), 4),\n",
       " (('j', 'c'), 4),\n",
       " (('y', 'w'), 4),\n",
       " (('v', 'k'), 3),\n",
       " (('x', 'w'), 3),\n",
       " (('t', 'j'), 3),\n",
       " (('c', 'j'), 3),\n",
       " (('q', 'w'), 3),\n",
       " (('g', 'b'), 3),\n",
       " (('o', 'q'), 3),\n",
       " (('r', 'x'), 3),\n",
       " (('d', 'c'), 3),\n",
       " (('g', 'j'), 3),\n",
       " (('x', 'f'), 3),\n",
       " (('z', 'w'), 3),\n",
       " (('d', 'k'), 3),\n",
       " (('u', 'u'), 3),\n",
       " (('m', 'v'), 3),\n",
       " (('c', 'x'), 3),\n",
       " (('l', 'q'), 3),\n",
       " (('p', 'b'), 2),\n",
       " (('t', 'g'), 2),\n",
       " (('q', 's'), 2),\n",
       " (('t', 'x'), 2),\n",
       " (('f', 'k'), 2),\n",
       " (('b', 't'), 2),\n",
       " (('j', 'n'), 2),\n",
       " (('k', 'c'), 2),\n",
       " (('z', 'k'), 2),\n",
       " (('s', 'j'), 2),\n",
       " (('s', 'f'), 2),\n",
       " (('z', 'j'), 2),\n",
       " (('n', 'q'), 2),\n",
       " (('f', 'z'), 2),\n",
       " (('h', 'g'), 2),\n",
       " (('w', 'w'), 2),\n",
       " (('k', 'j'), 2),\n",
       " (('j', 'k'), 2),\n",
       " (('w', 'm'), 2),\n",
       " (('z', 'c'), 2),\n",
       " (('z', 'v'), 2),\n",
       " (('w', 'f'), 2),\n",
       " (('q', 'm'), 2),\n",
       " (('k', 'z'), 2),\n",
       " (('j', 'j'), 2),\n",
       " (('z', 'p'), 2),\n",
       " (('j', 't'), 2),\n",
       " (('k', 'b'), 2),\n",
       " (('m', 'w'), 2),\n",
       " (('h', 'f'), 2),\n",
       " (('c', 'g'), 2),\n",
       " (('t', 'f'), 2),\n",
       " (('h', 'c'), 2),\n",
       " (('q', 'o'), 2),\n",
       " (('k', 'd'), 2),\n",
       " (('k', 'v'), 2),\n",
       " (('s', 'g'), 2),\n",
       " (('z', 'd'), 2),\n",
       " (('q', 'r'), 1),\n",
       " (('d', 'z'), 1),\n",
       " (('p', 'j'), 1),\n",
       " (('q', 'l'), 1),\n",
       " (('p', 'f'), 1),\n",
       " (('q', 'e'), 1),\n",
       " (('b', 'c'), 1),\n",
       " (('c', 'd'), 1),\n",
       " (('m', 'f'), 1),\n",
       " (('p', 'n'), 1),\n",
       " (('w', 'b'), 1),\n",
       " (('p', 'c'), 1),\n",
       " (('h', 'p'), 1),\n",
       " (('f', 'h'), 1),\n",
       " (('b', 'j'), 1),\n",
       " (('f', 'g'), 1),\n",
       " (('z', 'g'), 1),\n",
       " (('c', 'p'), 1),\n",
       " (('p', 'k'), 1),\n",
       " (('p', 'm'), 1),\n",
       " (('x', 'n'), 1),\n",
       " (('s', 'q'), 1),\n",
       " (('k', 'f'), 1),\n",
       " (('m', 'k'), 1),\n",
       " (('x', 'h'), 1),\n",
       " (('g', 'f'), 1),\n",
       " (('v', 'b'), 1),\n",
       " (('j', 'p'), 1),\n",
       " (('g', 'z'), 1),\n",
       " (('v', 'd'), 1),\n",
       " (('d', 'b'), 1),\n",
       " (('v', 'h'), 1),\n",
       " (('h', 'h'), 1),\n",
       " (('g', 'v'), 1),\n",
       " (('d', 'q'), 1),\n",
       " (('x', 'b'), 1),\n",
       " (('w', 'z'), 1),\n",
       " (('h', 'q'), 1),\n",
       " (('j', 'b'), 1),\n",
       " (('x', 'm'), 1),\n",
       " (('w', 'g'), 1),\n",
       " (('t', 'b'), 1),\n",
       " (('z', 'x'), 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = {} \n",
    "for w in words:\n",
    "    w = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(w, w[1:]):\n",
    "        biagram = (ch1, ch2)\n",
    "        b[biagram] = b.get(biagram,0) + 1 \n",
    "\n",
    "sorted(b.items(), key= lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/7b/7nq5283j24db505fwy_chq2r0000gn/T/ipykernel_3576/4265195184.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros(size=(len(stoi), len(stoi)), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n",
       "         1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n",
       "          134,  535,  929],\n",
       "        [6640,  556,  541,  470, 1042,  692,  134,  168, 2332, 1650,  175,  568,\n",
       "         2528, 1634, 5438,   63,   82,   60, 3264, 1118,  687,  381,  834,  161,\n",
       "          182, 2050,  435],\n",
       "        [ 114,  321,   38,    1,   65,  655,    0,    0,   41,  217,    1,    0,\n",
       "          103,    0,    4,  105,    0,    0,  842,    8,    2,   45,    0,    0,\n",
       "            0,   83,    0],\n",
       "        [  97,  815,    0,   42,    1,  551,    0,    2,  664,  271,    3,  316,\n",
       "          116,    0,    0,  380,    1,   11,   76,    5,   35,   35,    0,    0,\n",
       "            3,  104,    4],\n",
       "        [ 516, 1303,    1,    3,  149, 1283,    5,   25,  118,  674,    9,    3,\n",
       "           60,   30,   31,  378,    0,    1,  424,   29,    4,   92,   17,   23,\n",
       "            0,  317,    1],\n",
       "        [3983,  679,  121,  153,  384, 1271,   82,  125,  152,  818,   55,  178,\n",
       "         3248,  769, 2675,  269,   83,   14, 1958,  861,  580,   69,  463,   50,\n",
       "          132, 1070,  181],\n",
       "        [  80,  242,    0,    0,    0,  123,   44,    1,    1,  160,    0,    2,\n",
       "           20,    0,    4,   60,    0,    0,  114,    6,   18,   10,    0,    4,\n",
       "            0,   14,    2],\n",
       "        [ 108,  330,    3,    0,   19,  334,    1,   25,  360,  190,    3,    0,\n",
       "           32,    6,   27,   83,    0,    0,  201,   30,   31,   85,    1,   26,\n",
       "            0,   31,    1],\n",
       "        [2409, 2244,    8,    2,   24,  674,    2,    2,    1,  729,    9,   29,\n",
       "          185,  117,  138,  287,    1,    1,  204,   31,   71,  166,   39,   10,\n",
       "            0,  213,   20],\n",
       "        [2489, 2445,  110,  509,  440, 1653,  101,  428,   95,   82,   76,  445,\n",
       "         1345,  427, 2126,  588,   53,   52,  849, 1316,  541,  109,  269,    8,\n",
       "           89,  779,  277],\n",
       "        [  71, 1473,    1,    4,    4,  440,    0,    0,   45,  119,    2,    2,\n",
       "            9,    5,    2,  479,    1,    0,   11,    7,    2,  202,    5,    6,\n",
       "            0,   10,    0],\n",
       "        [ 363, 1731,    2,    2,    2,  895,    1,    0,  307,  509,    2,   20,\n",
       "          139,    9,   26,  344,    0,    0,  109,   95,   17,   50,    2,   34,\n",
       "            0,  379,    2],\n",
       "        [1314, 2623,   52,   25,  138, 2921,   22,    6,   19, 2480,    6,   24,\n",
       "         1345,   60,   14,  692,   15,    3,   18,   94,   77,  324,   72,   16,\n",
       "            0, 1588,   10],\n",
       "        [ 516, 2590,  112,   51,   24,  818,    1,    0,    5, 1256,    7,    1,\n",
       "            5,  168,   20,  452,   38,    0,   97,   35,    4,  139,    3,    2,\n",
       "            0,  287,   11],\n",
       "        [6763, 2977,    8,  213,  704, 1359,   11,  273,   26, 1725,   44,   58,\n",
       "          195,   19, 1906,  496,    5,    2,   44,  278,  443,   96,   55,   11,\n",
       "            6,  465,  145],\n",
       "        [ 855,  149,  140,  114,  190,  132,   34,   44,  171,   69,   16,   68,\n",
       "          619,  261, 2411,  115,   95,    3, 1059,  504,  118,  275,  176,  114,\n",
       "           45,  103,   54],\n",
       "        [  33,  209,    2,    1,    0,  197,    1,    0,  204,   61,    1,    1,\n",
       "           16,    1,    1,   59,   39,    0,  151,   16,   17,    4,    0,    0,\n",
       "            0,   12,    0],\n",
       "        [  28,   13,    0,    0,    0,    1,    0,    0,    0,   13,    0,    0,\n",
       "            1,    2,    0,    2,    0,    0,    1,    2,    0,  206,    0,    3,\n",
       "            0,    0,    0],\n",
       "        [1377, 2356,   41,   99,  187, 1697,    9,   76,  121, 3033,   25,   90,\n",
       "          413,  162,  140,  869,   14,   16,  425,  190,  208,  252,   80,   21,\n",
       "            3,  773,   23],\n",
       "        [1169, 1201,   21,   60,    9,  884,    2,    2, 1285,  684,    2,   82,\n",
       "          279,   90,   24,  531,   51,    1,   55,  461,  765,  185,   14,   24,\n",
       "            0,  215,   10],\n",
       "        [ 483, 1027,    1,   17,    0,  716,    2,    2,  647,  532,    3,    0,\n",
       "          134,    4,   22,  667,    0,    0,  352,   35,  374,   78,   15,   11,\n",
       "            2,  341,  105],\n",
       "        [ 155,  163,  103,  103,  136,  169,   19,   47,   58,  121,   14,   93,\n",
       "          301,  154,  275,   10,   16,   10,  414,  474,   82,    3,   37,   86,\n",
       "           34,   13,   45],\n",
       "        [  88,  642,    1,    0,    1,  568,    0,    0,    1,  911,    0,    3,\n",
       "           14,    0,    8,  153,    0,    0,   48,    0,    0,    7,    7,    0,\n",
       "            0,  121,    0],\n",
       "        [  51,  280,    1,    0,    8,  149,    2,    1,   23,  148,    0,    6,\n",
       "           13,    2,   58,   36,    0,    0,   22,   20,    8,   25,    0,    2,\n",
       "            0,   73,    1],\n",
       "        [ 164,  103,    1,    4,    5,   36,    3,    0,    1,  102,    0,    0,\n",
       "           39,    1,    1,   41,    0,    0,    0,   31,   70,    5,    0,    3,\n",
       "           38,   30,   19],\n",
       "        [2007, 2143,   27,  115,  272,  301,   12,   30,   22,  192,   23,   86,\n",
       "         1104,  148, 1826,  271,   15,    6,  291,  401,  104,  141,  106,    4,\n",
       "           28,   23,   78],\n",
       "        [ 160,  860,    4,    2,    2,  373,    0,    1,   43,  364,    2,    2,\n",
       "          123,   35,    4,  110,    2,    0,   32,    4,    4,   73,    2,    3,\n",
       "            1,  147,   45]], dtype=torch.int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for w in words:\n",
    "    w = ['*'] + list(w) + ['*']\n",
    "    for ch1, ch2 in zip(w, w[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        N[ix1, ix2] += 1 \n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m16\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOranges\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(stoi)):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(stoi)):\n",
      "File \u001b[0;32m~/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/matplotlib/pyplot.py:3562\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   3541\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[1;32m   3542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mimshow\u001b[39m(\n\u001b[1;32m   3543\u001b[0m     X: ArrayLike \u001b[38;5;241m|\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3560\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3561\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AxesImage:\n\u001b[0;32m-> 3562\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3566\u001b[0m \u001b[43m        \u001b[49m\u001b[43maspect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3567\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3568\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3571\u001b[0m \u001b[43m        \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3573\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilternorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilterrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3577\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3578\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3579\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3580\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3581\u001b[0m     sci(__ret)\n\u001b[1;32m   3582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m~/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/matplotlib/__init__.py:1476\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1475\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1476\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1477\u001b[0m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1478\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1479\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1481\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1482\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1483\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/matplotlib/axes/_axes.py:5895\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5893\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m-> 5895\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5896\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5898\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/matplotlib/image.py:729\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    728\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/matplotlib/image.py:690\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_normalize_image_array\u001b[39m(A):\n\u001b[1;32m    686\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;124;03m    Check validity of image-like input *A* and normalize it to a format suitable for\u001b[39;00m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;124;03m    Image subclasses.\u001b[39;00m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 690\u001b[0m     A \u001b[38;5;241m=\u001b[39m \u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_masked_invalid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39muint8 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mcan_cast(A\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame_kind\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    692\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage data of dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    693\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconverted to float\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/matplotlib/cbook.py:733\u001b[0m, in \u001b[0;36msafe_masked_invalid\u001b[0;34m(x, copy)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msafe_masked_invalid\u001b[39m(x, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 733\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39misnative:\n\u001b[1;32m    735\u001b[0m         \u001b[38;5;66;03m# If we have already made a copy, do the byteswap in place, else make a\u001b[39;00m\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;66;03m# copy with the byte order swapped.\u001b[39;00m\n\u001b[1;32m    737\u001b[0m         \u001b[38;5;66;03m# Swap to native order.\u001b[39;00m\n\u001b[1;32m    738\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mbyteswap(inplace\u001b[38;5;241m=\u001b[39mcopy)\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mnewbyteorder(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/torch/_tensor.py:1062\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1062\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x1600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(N, cmap='Oranges')\n",
    "for i in range(len(stoi)):\n",
    "    for j in range(len(stoi)):\n",
    "        chstr = itos[i] + itos[j]\n",
    "        plt.text(j, i, chstr, ha='center', va='bottom', color='gray')\n",
    "        plt.text(j, i, N[i,j].item(), ha='center', va='top', color='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*junide*\n",
      "*janasah*\n",
      "*p*\n",
      "*cony*\n",
      "*a*\n",
      "*nn*\n",
      "*kohin*\n",
      "*tolian*\n",
      "*juee*\n",
      "*ksahnaauranilevias*\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for _ in range(10):\n",
    "    ix = 0\n",
    "    words = ['*'] \n",
    "    while True: \n",
    "        p = N[ix].float() \n",
    "        p = p / p.sum() \n",
    "        # p = torch.ones(27) / 27.\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        char = itos[ix]\n",
    "        words.append(char)\n",
    "        if char == '*':\n",
    "            break \n",
    "    print(''.join(words) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1950, 0.0332, 0.0059, 0.0075, 0.0188, 0.0622, 0.0040, 0.0061, 0.0074,\n",
       "         0.0401, 0.0027, 0.0087, 0.1590, 0.0377, 0.1310, 0.0132, 0.0041, 0.0007,\n",
       "         0.0959, 0.0422, 0.0284, 0.0034, 0.0227, 0.0024, 0.0065, 0.0524, 0.0089]),\n",
       " torch.Size([27]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = N[5]/N[5].sum(dim=0,keepdim=True)\n",
    "p, p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 22, 22])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.multinomial(, num_samples=3, replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = {} \n",
    "mem = torch.multinomial(p, num_samples=200, replacement=True, generator=g)\n",
    "for ix in mem:\n",
    "    i = ix.item()\n",
    "    b[i] = b.get(i,0) + 1\n",
    "values = list(b.values() )\n",
    "sum([value / sum(values) for value in values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 65), (0, 116), (2, 19)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/7b/7nq5283j24db505fwy_chq2r0000gn/T/ipykernel_9726/282209132.py\", line 1, in <module>\n",
      "    from makemore import Biagrams\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/Makemore/makemore.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "from makemore import Biagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biagram = Biagrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biagram.open('names.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biagram.train_model(show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*junide*janasah*p*cony*'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biagram.generate_words(num_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*a, prob=0.1377, log=-1.9829\n",
      "al, prob=0.0746, log=-2.5955\n",
      "li, prob=0.1777, log=-1.7278\n",
      "ic, prob=0.0288, log=-3.5489\n",
      "ci, prob=0.0767, log=-2.5675\n",
      "iq, prob=0.0029, log=-5.8301\n",
      "qj, prob=0.0000, log=-inf\n",
      "j*, prob=0.0245, log=-3.7098\n",
      "neg likelihood=inf\n"
     ]
    }
   ],
   "source": [
    "biagram.return_probabilities(word='aliciqj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biagram.P[biagram.stoi['j'], biagram.stoi['q']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "xs = [] \n",
    "ys = [] \n",
    "word = biagram.words[:1]\n",
    "for w in word: \n",
    "    w = ['*'] + list(w) + ['*']\n",
    "    for ch1, ch2 in zip(w,w[1:]):\n",
    "        ix1 = biagram.stoi[ch1]\n",
    "        ix2 = biagram.stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "xs = torch.tensor(xs) \n",
    "ys = torch.tensor(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.8068, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F \n",
    "xenc = F.one_hot(xs,num_classes=len(biagram.abc)).float()\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn(size=(len(biagram.abc), len(biagram.abc)), generator=g, requires_grad=True)\n",
    "(xenc @ W)[4, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.8068)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc[4] * W[:,9]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/7b/7nq5283j24db505fwy_chq2r0000gn/T/ipykernel_10320/2572114059.py\", line 1, in <module>\n",
      "    from makemore import NNBiagrams\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/Makemore/makemore.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/vicentearjona/Documents/Spelled-out intro to Lang Models/.venv/lib/python3.9/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "from makemore import NNBiagrams\n",
    "\n",
    "biagram = NNBiagrams() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biagram.open(file='names.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration = 0. Loss = 3.807279109954834\n",
      "iteration = 1. Loss = 3.7609736919403076\n",
      "iteration = 2. Loss = 3.717297315597534\n",
      "iteration = 3. Loss = 3.6759932041168213\n",
      "iteration = 4. Loss = 3.6368601322174072\n",
      "iteration = 5. Loss = 3.5997345447540283\n",
      "iteration = 6. Loss = 3.5644800662994385\n",
      "iteration = 7. Loss = 3.53098201751709\n",
      "iteration = 8. Loss = 3.4991378784179688\n",
      "iteration = 9. Loss = 3.4688587188720703\n",
      "iteration = 10. Loss = 3.440062999725342\n",
      "iteration = 11. Loss = 3.41267466545105\n",
      "iteration = 12. Loss = 3.3866212368011475\n",
      "iteration = 13. Loss = 3.361834764480591\n",
      "iteration = 14. Loss = 3.3382480144500732\n",
      "iteration = 15. Loss = 3.315795421600342\n",
      "iteration = 16. Loss = 3.2944135665893555\n",
      "iteration = 17. Loss = 3.274040460586548\n",
      "iteration = 18. Loss = 3.2546157836914062\n",
      "iteration = 19. Loss = 3.236081838607788\n",
      "iteration = 20. Loss = 3.2183830738067627\n",
      "iteration = 21. Loss = 3.2014665603637695\n",
      "iteration = 22. Loss = 3.1852829456329346\n",
      "iteration = 23. Loss = 3.169786214828491\n",
      "iteration = 24. Loss = 3.1549324989318848\n",
      "iteration = 25. Loss = 3.1406822204589844\n",
      "iteration = 26. Loss = 3.1269989013671875\n",
      "iteration = 27. Loss = 3.113847255706787\n",
      "iteration = 28. Loss = 3.1011977195739746\n",
      "iteration = 29. Loss = 3.089020252227783\n",
      "iteration = 30. Loss = 3.0772898197174072\n",
      "iteration = 31. Loss = 3.065980911254883\n",
      "iteration = 32. Loss = 3.055072784423828\n",
      "iteration = 33. Loss = 3.044543504714966\n",
      "iteration = 34. Loss = 3.0343751907348633\n",
      "iteration = 35. Loss = 3.0245494842529297\n",
      "iteration = 36. Loss = 3.0150513648986816\n",
      "iteration = 37. Loss = 3.005863904953003\n",
      "iteration = 38. Loss = 2.996973991394043\n",
      "iteration = 39. Loss = 2.988368511199951\n",
      "iteration = 40. Loss = 2.9800338745117188\n",
      "iteration = 41. Loss = 2.9719598293304443\n",
      "iteration = 42. Loss = 2.9641337394714355\n",
      "iteration = 43. Loss = 2.95654559135437\n",
      "iteration = 44. Loss = 2.949186086654663\n",
      "iteration = 45. Loss = 2.942044973373413\n",
      "iteration = 46. Loss = 2.9351136684417725\n",
      "iteration = 47. Loss = 2.9283835887908936\n",
      "iteration = 48. Loss = 2.921846628189087\n",
      "iteration = 49. Loss = 2.915494918823242\n",
      "iteration = 50. Loss = 2.9093213081359863\n",
      "iteration = 51. Loss = 2.9033188819885254\n",
      "iteration = 52. Loss = 2.8974804878234863\n",
      "iteration = 53. Loss = 2.8918001651763916\n",
      "iteration = 54. Loss = 2.8862717151641846\n",
      "iteration = 55. Loss = 2.8808887004852295\n",
      "iteration = 56. Loss = 2.8756461143493652\n",
      "iteration = 57. Loss = 2.8705389499664307\n",
      "iteration = 58. Loss = 2.8655617237091064\n",
      "iteration = 59. Loss = 2.8607096672058105\n",
      "iteration = 60. Loss = 2.855978012084961\n",
      "iteration = 61. Loss = 2.851362705230713\n",
      "iteration = 62. Loss = 2.8468587398529053\n",
      "iteration = 63. Loss = 2.8424630165100098\n",
      "iteration = 64. Loss = 2.8381712436676025\n",
      "iteration = 65. Loss = 2.833979606628418\n",
      "iteration = 66. Loss = 2.8298847675323486\n",
      "iteration = 67. Loss = 2.825883150100708\n",
      "iteration = 68. Loss = 2.8219714164733887\n",
      "iteration = 69. Loss = 2.8181471824645996\n",
      "iteration = 70. Loss = 2.8144071102142334\n",
      "iteration = 71. Loss = 2.810748338699341\n",
      "iteration = 72. Loss = 2.8071675300598145\n",
      "iteration = 73. Loss = 2.8036632537841797\n",
      "iteration = 74. Loss = 2.800232410430908\n",
      "iteration = 75. Loss = 2.79687237739563\n",
      "iteration = 76. Loss = 2.793581962585449\n",
      "iteration = 77. Loss = 2.7903575897216797\n",
      "iteration = 78. Loss = 2.787198066711426\n",
      "iteration = 79. Loss = 2.7841014862060547\n",
      "iteration = 80. Loss = 2.7810654640197754\n",
      "iteration = 81. Loss = 2.778088331222534\n",
      "iteration = 82. Loss = 2.7751684188842773\n",
      "iteration = 83. Loss = 2.7723045349121094\n",
      "iteration = 84. Loss = 2.76949405670166\n",
      "iteration = 85. Loss = 2.7667360305786133\n",
      "iteration = 86. Loss = 2.764028549194336\n",
      "iteration = 87. Loss = 2.76137113571167\n",
      "iteration = 88. Loss = 2.758761167526245\n",
      "iteration = 89. Loss = 2.7561984062194824\n",
      "iteration = 90. Loss = 2.753681182861328\n",
      "iteration = 91. Loss = 2.7512080669403076\n",
      "iteration = 92. Loss = 2.7487781047821045\n",
      "iteration = 93. Loss = 2.7463903427124023\n",
      "iteration = 94. Loss = 2.7440433502197266\n",
      "iteration = 95. Loss = 2.74173641204834\n",
      "iteration = 96. Loss = 2.7394683361053467\n",
      "iteration = 97. Loss = 2.7372384071350098\n",
      "iteration = 98. Loss = 2.7350451946258545\n",
      "iteration = 99. Loss = 2.7328882217407227\n",
      "iteration = 100. Loss = 2.730766534805298\n",
      "iteration = 101. Loss = 2.7286791801452637\n",
      "iteration = 102. Loss = 2.726625442504883\n",
      "iteration = 103. Loss = 2.724604606628418\n",
      "iteration = 104. Loss = 2.722616195678711\n",
      "iteration = 105. Loss = 2.720658779144287\n",
      "iteration = 106. Loss = 2.7187318801879883\n",
      "iteration = 107. Loss = 2.7168354988098145\n",
      "iteration = 108. Loss = 2.714967966079712\n",
      "iteration = 109. Loss = 2.7131295204162598\n",
      "iteration = 110. Loss = 2.7113189697265625\n",
      "iteration = 111. Loss = 2.709536075592041\n",
      "iteration = 112. Loss = 2.707779884338379\n",
      "iteration = 113. Loss = 2.706050395965576\n",
      "iteration = 114. Loss = 2.7043466567993164\n",
      "iteration = 115. Loss = 2.7026681900024414\n",
      "iteration = 116. Loss = 2.701014518737793\n",
      "iteration = 117. Loss = 2.699385166168213\n",
      "iteration = 118. Loss = 2.697779655456543\n",
      "iteration = 119. Loss = 2.696197509765625\n",
      "iteration = 120. Loss = 2.6946380138397217\n",
      "iteration = 121. Loss = 2.693101167678833\n",
      "iteration = 122. Loss = 2.6915862560272217\n",
      "iteration = 123. Loss = 2.6900930404663086\n",
      "iteration = 124. Loss = 2.6886210441589355\n",
      "iteration = 125. Loss = 2.6871695518493652\n",
      "iteration = 126. Loss = 2.6857388019561768\n",
      "iteration = 127. Loss = 2.6843278408050537\n",
      "iteration = 128. Loss = 2.682936668395996\n",
      "iteration = 129. Loss = 2.681565046310425\n",
      "iteration = 130. Loss = 2.6802120208740234\n",
      "iteration = 131. Loss = 2.678877830505371\n",
      "iteration = 132. Loss = 2.6775615215301514\n",
      "iteration = 133. Loss = 2.6762630939483643\n",
      "iteration = 134. Loss = 2.674983263015747\n",
      "iteration = 135. Loss = 2.673719882965088\n",
      "iteration = 136. Loss = 2.672473430633545\n",
      "iteration = 137. Loss = 2.6712441444396973\n",
      "iteration = 138. Loss = 2.6700310707092285\n",
      "iteration = 139. Loss = 2.6688342094421387\n",
      "iteration = 140. Loss = 2.6676533222198486\n",
      "iteration = 141. Loss = 2.666487693786621\n",
      "iteration = 142. Loss = 2.665337324142456\n",
      "iteration = 143. Loss = 2.6642024517059326\n",
      "iteration = 144. Loss = 2.6630821228027344\n",
      "iteration = 145. Loss = 2.6619763374328613\n",
      "iteration = 146. Loss = 2.6608850955963135\n",
      "iteration = 147. Loss = 2.6598074436187744\n",
      "iteration = 148. Loss = 2.6587436199188232\n",
      "iteration = 149. Loss = 2.657693862915039\n",
      "iteration = 150. Loss = 2.6566576957702637\n",
      "iteration = 151. Loss = 2.6556339263916016\n",
      "iteration = 152. Loss = 2.65462327003479\n",
      "iteration = 153. Loss = 2.653625726699829\n",
      "iteration = 154. Loss = 2.6526403427124023\n",
      "iteration = 155. Loss = 2.6516671180725098\n",
      "iteration = 156. Loss = 2.6507065296173096\n",
      "iteration = 157. Loss = 2.6497573852539062\n",
      "iteration = 158. Loss = 2.648820161819458\n",
      "iteration = 159. Loss = 2.6478941440582275\n",
      "iteration = 160. Loss = 2.646979808807373\n",
      "iteration = 161. Loss = 2.6460769176483154\n",
      "iteration = 162. Loss = 2.6451845169067383\n",
      "iteration = 163. Loss = 2.6443028450012207\n",
      "iteration = 164. Loss = 2.643432378768921\n",
      "iteration = 165. Loss = 2.6425719261169434\n",
      "iteration = 166. Loss = 2.641721725463867\n",
      "iteration = 167. Loss = 2.6408820152282715\n",
      "iteration = 168. Loss = 2.640052080154419\n",
      "iteration = 169. Loss = 2.6392316818237305\n",
      "iteration = 170. Loss = 2.6384212970733643\n",
      "iteration = 171. Loss = 2.637620210647583\n",
      "iteration = 172. Loss = 2.636828899383545\n",
      "iteration = 173. Loss = 2.6360464096069336\n",
      "iteration = 174. Loss = 2.635272979736328\n",
      "iteration = 175. Loss = 2.6345086097717285\n",
      "iteration = 176. Loss = 2.6337532997131348\n",
      "iteration = 177. Loss = 2.6330065727233887\n",
      "iteration = 178. Loss = 2.632268190383911\n",
      "iteration = 179. Loss = 2.631537914276123\n",
      "iteration = 180. Loss = 2.6308162212371826\n",
      "iteration = 181. Loss = 2.6301028728485107\n",
      "iteration = 182. Loss = 2.6293978691101074\n",
      "iteration = 183. Loss = 2.628700017929077\n",
      "iteration = 184. Loss = 2.6280100345611572\n",
      "iteration = 185. Loss = 2.6273276805877686\n",
      "iteration = 186. Loss = 2.6266531944274902\n",
      "iteration = 187. Loss = 2.625986099243164\n",
      "iteration = 188. Loss = 2.62532639503479\n",
      "iteration = 189. Loss = 2.624673843383789\n",
      "iteration = 190. Loss = 2.624028205871582\n",
      "iteration = 191. Loss = 2.623389720916748\n",
      "iteration = 192. Loss = 2.622758626937866\n",
      "iteration = 193. Loss = 2.622133255004883\n",
      "iteration = 194. Loss = 2.6215152740478516\n",
      "iteration = 195. Loss = 2.6209042072296143\n",
      "iteration = 196. Loss = 2.6202993392944336\n",
      "iteration = 197. Loss = 2.6197006702423096\n",
      "iteration = 198. Loss = 2.6191086769104004\n",
      "iteration = 199. Loss = 2.6185226440429688\n"
     ]
    }
   ],
   "source": [
    "biagram.gradient_descent(num_iter=200, lr=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'juwjde',\n",
       " 'janaqah',\n",
       " 'pxzfby',\n",
       " 'a',\n",
       " 'nn',\n",
       " 'kai',\n",
       " 'ritolian',\n",
       " 'jgezzusahnaauyanilevias',\n",
       " 'dbdainrwieta',\n",
       " 'sejaielylarte',\n",
       " 'ffrmumerifon',\n",
       " 'mmjan',\n",
       " 'nnslenariani',\n",
       " 'core',\n",
       " 'yaenon',\n",
       " 'ka',\n",
       " 'jabdinerimiin',\n",
       " 'wynin',\n",
       " 'anaasn',\n",
       " 'sviihonszxhddgon',\n",
       " 'mitpanil',\n",
       " '',\n",
       " 'ripann',\n",
       " 'that',\n",
       " 'jkareli',\n",
       " 'isa',\n",
       " 'dyn',\n",
       " 'rmjelujamahauywyalevma',\n",
       " 'crarr',\n",
       " 'jdkh',\n",
       " '']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(biagram.generate_words(num_words=30)).split('*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "one_hot(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m \n\u001b[1;32m      3\u001b[0m ix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[0;32m----> 4\u001b[0m xenc \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mix\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbiagram\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m      5\u001b[0m counts \u001b[38;5;241m=\u001b[39m (xenc \u001b[38;5;241m@\u001b[39m biagram\u001b[38;5;241m.\u001b[39mW)\u001b[38;5;241m.\u001b[39mexp()\n\u001b[1;32m      6\u001b[0m p \u001b[38;5;241m=\u001b[39m counts \u001b[38;5;241m/\u001b[39m counts\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: one_hot(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F \n",
    "ix = 0 \n",
    "xenc = F.one_hot([ix], num_classes=biagram.dim).float()\n",
    "counts = (xenc @ biagram.W).exp()\n",
    "p = counts / counts.sum(dim=1, keepdim=True)\n",
    "ix = torch.multinomial(\n",
    "    input=p, num_samples=1, replacement=True, generator=g\n",
    ").item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
